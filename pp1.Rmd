---
title: "Practical Machine Learning Course, Final Project Write Up"
author: "Elenssar (R. L. D.) <elenssar21@gmail.com>"
output: html_document
---
Project Objective: Analysis of Fitness Exercise Correctness problem and building of a predictive model.

### Loading and Preparing the Data set
Loading the dataset into training and testing variables. The analysis will be done over the trainingOrinal set.
```{r, message=FALSE}
library(gdata)
traindataOriginal = read.csv("pml-training.csv")
testingdataOriginal = read.csv("pml-testing.csv")
```
As the objective of the predictive model is how well anybody do an activity, in general, based on the accelerometer and dumbell results, the user name won't be taked in count as a predictor.
In this sense, the time is not adding any extra valuable information `cvtd_timestamp`, and as we have the classification (prediction) by single movement event, the time window variables`raw_timestamp_part_1, raw_timestamp_part_2, new_window, num_window` can be avoid too, as they are useful if we are going to sumarize (average, media) the sensors predictors results by fragments of time.
The first reduction of predictors:
```{r, message=FALSE}
trainingReduced<-traindataOriginal[,8:160]
testingReduced<-testingdataOriginal[,8:160]
```
###Analysing Raw data
As we still have 152 features, lets analyze them to reduce this set into a number of predictors that give the most information possible to the result.
For this task, would be a good start to have a look to values, using summary command: 
```{r, message=FALSE}
sm<-summary(trainingReduced)
``` 
For space, I will display only the first 10 columns
```{r}
sm[,1:10]
```
As we can see in the result, some of the predictors does not have values  (0, null, or zero division), as `kurtosis_yaw_belt`, and these do not add any information to the prediction. Others are highly unbalanced, as only an small set of them has values, as `kurtosis_picth_belt, skewness_roll_belt, skewness_roll_belt.1`. We do not want them because they can produce an undue influence on the model, or add extra no worth calculations. More further, if we will use deicision trees, as in this clasification case seems more appropiate, balanced predictors will be my first choice as they will split better and faster the whole data. This kind of predictors are called "near zero variance", and we can make use of `nearZeroVar` function from caret library to detect all of them. 
```{r, message=FALSE}
library(caret)
nzv <- nearZeroVar(trainingReduced, saveMetrics = TRUE)
```
The 10 first columns looks like:
```{r}
nzv[1:10,]
```
And we can verify which predictors are near zero variables, by looking the `nzv` column with TRUE value. We can select the variables which are best balanced, no near zero variance.
```{r}
nzv <- nearZeroVar(trainingReduced)
filteredTraining <- trainingReduced[, -nzv]
dim(filteredTraining)
summary(filteredTraining[,5])
filteredTesting <- testingReduced[, -nzv]
```
The features have been reduced to 94 features, 93 balanced predictors plus the resulting class.
The function does not use factor `NA` in the calculation of the variance, so the unbalanced predictors that have a high number of samples with `NA` values, as `max_roll_belt`, have to be eliminated manually.
 
```{r}
unbalancedNA<-c(5:20, 34, 44:47, 51:56, 58:67, 80:82, 84)
filteredTraining<-filteredTraining[,-unbalancedNA]
dim(filteredTraining)
filteredTesting<-filteredTesting[,-unbalancedNA]
``` 
### Building the Prediction Model
As a clasification problem, the decision trees can be used, and we have a set of balanced features that can be used to split the samples. As at these point we have a high number of features, we can take advantage of the boostrap step (resampling) of the Random Forest for the selection of the most important variables and final model building. The reduced set of features is 53 factors.

```{r, message=FALSE}
library(randomForest)
set.seed(33832)
```
First I will split the data, to have a validation set.
```{r}
inTrain <- createDataPartition(filteredTraining$classe, p=0.70, list=FALSE)
training <- filteredTraining[inTrain, ]
validating <- filteredTraining[-inTrain, ]
```
Training the model, with 1000 trees:
```{r}
modelFit <- randomForest(classe~.,data=training, ntree = 1000)
print(modelFit)
```
Only a few samples are missclasificated as we can see in the confusion matrix and the out-of-bag error is as low as 0.52%. We can have a have a look at the variable importance estimates obtained.

```{r}
impVars <- varImp(modelFit)
impVars
```
### Validation and Testing Evaluation

The prediction Resulting with the validation data:

```{r}
pred<-predict(modelFit, validating)
validating$predRight<-pred==validating$classe
table(pred,validating$classe)
```


Prediction resulting with the project testing data:

```{r}
predT<-predict(modelFit, filteredTesting)
```